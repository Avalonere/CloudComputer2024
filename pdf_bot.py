import os
import re

from collections import Counter
import streamlit as st
from PyPDF2 import PdfReader
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

from DBtest import WordWiseDB
from chains import load_llm
from dotenv import load_dotenv

from sqlite import exec_query
import nltk
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet
import time
from streamlit_modal import Modal
import requests

from docx import Document
import hashlib

#åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(".env")
ollama_base_url = os.getenv("OLLAMA_BASE_URL")
llm_name = os.getenv("LLM")
neo4j_uri = os.getenv("NEO4J_URI")
neo4j_usr = os.getenv("NEO4J_USERNAME")
neo4j_passwd = os.getenv("NEO4J_PASSWORD")
#è¿æ¥neo4jæ•°æ®åº“
db = WordWiseDB(uri=neo4j_uri, user=neo4j_usr, password=neo4j_passwd)


class StreamHandler(BaseCallbackHandler):
    """åˆ›å»ºæ–°çš„å•è¯åˆ—è¡¨

            Args:
                wid (str): è¯å•å”¯ä¸€æ ‡è¯†ç¬¦
                name (str): è¯å•åç§°
                description (str): è¯å•æè¿°
                owner_uid (str): åˆ›å»ºè€…çš„ç”¨æˆ·ID

            Note:
                ä¼šå»ºç«‹ç”¨æˆ·ä¸è¯å•ä¹‹é—´çš„ OWNS å…³ç³»
            """
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text
        self.complete_text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.complete_text += token

    def get_text(self):
        return self.complete_text


llm = load_llm(llm_name, config={"ollama_base_url": ollama_base_url})
memory = ConversationBufferMemory(return_messages=True)
conversation = ConversationChain(llm=llm, memory=memory)
st.set_page_config(page_title="PDFè¯æ±‡å­¦ä¹ åŠ©æ‰‹", layout="wide")





def ReadLinesAsList(file):
    """è¯»å–å•è¯è¡¨æ–‡ä»¶ä¸­çš„å•è¯

    å› ä¸ºæ–‡ä»¶æ¯è¡Œåªæœ‰ä¸€ä¸ªå•è¯ï¼Œè¯»å–æ¯ä¸€è¡Œçš„å†…å®¹å¹¶å»é™¤ç©ºæ ¼ï¼Œ
    ç„¶åæ”¾å…¥å­—å…¸ä¸­ã€‚

            Args:
               file:æ–‡ä»¶è·¯å¾„

            Returns:
                vocab:æ–‡ä»¶ä¸­å•è¯ç»„æˆçš„å­—å…¸
    """
    with open(file, 'r') as f:
        lines = f.readlines()
        vocab = {word.strip().split()[0].lower()
                 for word in lines if word.strip()}

        return vocab


# é¢„åŠ è½½è¯æ±‡è¡¨
@st.cache_data
def load_vocab_tables():
    """é¢„åŠ è½½CET6å’ŒCOCAè¯æ±‡è¡¨"""

    cet6_path = "/app/CET_4_6_edited.txt"
    coca_path = "/app/COCA_20000.txt"

    #ä»æ–‡ä»¶ä¸­è¯»å–å•è¯è¡¨
    cet6_vocab = ReadLinesAsList(cet6_path)
    coca_vocab = ReadLinesAsList(coca_path)

    return cet6_vocab, coca_vocab


# åŠ è½½è¯æ±‡è¡¨ï¼Œå­˜å‚¨åˆ°å…¨å±€å˜é‡é‡Œ
CET6_VOCAB, COCA_VOCAB = load_vocab_tables()
wnl = WordNetLemmatizer()





def get_word_category(tag):
    #æ ¹æ®tagåˆ¤æ–­è¯æ€§
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.VERB


def is_valid_word(word):
    """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆå•è¯"""
    word = word.lower()
    # è¿‡æ»¤ä¸“æœ‰åè¯å’ŒéCOCAè¯è¡¨å•è¯
    if not word in COCA_VOCAB or not word.isalpha():
        # å¿…é¡»å…¨æ˜¯å­—æ¯
        return False

    return True


def extract_words(text):
    """æå–å¹¶å¤„ç†æ–‡æœ¬ä¸­çš„å•è¯"""
    words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
    valid_words = []
    for word in words:
        if len(word) > 1:  # è¿‡æ»¤å•å­—æ¯
            # è¯å½¢è¿˜åŸ
            if is_valid_word(word):
                valid_words.append(wnl.lemmatize(word, get_word_category(pos_tag([word])[0][1])))
    return valid_words


def display_word_card(word, explanation, freq, index, total,support_add):
    """æ˜¾ç¤ºå•è¯å¡ç‰‡"""

    #å®šä¹‰æ ·å¼
    st.markdown("""
    <style>
    .word-card {
        padding: 20px;
        border-radius: 10px;
        background-color: #f0f2f6;
        margin: 10px 0;
    }
    </style>
    """, unsafe_allow_html=True)

    with st.container():
        #æ˜¾ç¤ºå•è¯ã€é¢‘æ¬¡ã€é‡Šä¹‰
        st.markdown(f"<div class='word-card'>", unsafe_allow_html=True)
        st.markdown(f"### {word}")
        st.caption(f"åœ¨æ–‡æ¡£ä¸­å‡ºç° {freq} æ¬¡")
        st.markdown(explanation)
        st.caption(f"å¡ç‰‡ {index + 1}/{total}")
        st.markdown("</div>", unsafe_allow_html=True)


        if support_add:

            col1 , col2 = st.columns([2,1])
            with col1:
                # ä»é€‰æ‹©æ¡†ä¸­é€‰æ‹©å·²åˆ›å»ºçš„å•è¯è¡¨
                option = st.selectbox("é€‰æ‹©å•è¯è¡¨", ["CET6"]) 
                

            with col2:
                st.write("")
                st.write("")
                # ç‚¹å‡»æŒ‰é’®ï¼Œæ ¹æ®é€‰æ‹©çš„å•è¯è¡¨nameï¼Œåœ¨session_stateä¸­æŸ¥è¯¢å¯¹åº”å•è¯è¡¨çš„id
                if st.button("åŠ å…¥ç”Ÿè¯åº“"):
                    if option is not None:
                        #å°†å•è¯åŠ å…¥æŒ‡å®šidçš„å•è¯è¡¨ä¸­
                        st.success("æ·»åŠ æˆåŠŸï¼âœ…")





def get_word_explanations(words, llm):
    """ä½¿ç”¨LLMç”Ÿæˆå•è¯é‡Šä¹‰"""
    explanations = {}
    prompt_template = """è¯·ä¸ºä»¥ä¸‹è‹±è¯­å•è¯æä¾›ç®€æ˜çš„ä¸­æ–‡é‡Šä¹‰å’Œä¸€ä¸ªç®€çŸ­çš„ä¾‹å¥:
    å•è¯: {word}
    è¦æ±‚:
    1. ç»™å‡ºæœ€å¸¸ç”¨çš„1-2ä¸ªå«ä¹‰
    2. æä¾›ä¸€ä¸ªç®€å•çš„ä¾‹å¥
    3. è¾“å‡ºæ ¼å¼:
       é‡Šä¹‰: [ä¸­æ–‡é‡Šä¹‰]
       ä¾‹å¥: [è‹±æ–‡ä¾‹å¥]"""

    for word in words:
        container = st.empty()
        stream_handler = StreamHandler(container)
        prompt = prompt_template.format(word=word)
        llm.predict(prompt, callbacks=[stream_handler])
        explanations[word] = stream_handler.get_text()
        container.empty()  # æ¸…é™¤ä¸´æ—¶æ˜¾ç¤º
    return explanations

def get_doc_difficulty(text,llm):
    """ä½¿ç”¨LLMåˆ†ææ–‡æ¡£éš¾åº¦"""
    prompt_template = """è¯·ä»è¯æ±‡å’Œè¯­æ³•çš„è§’åº¦åˆ†æä¸‹é¢è¿™ç¯‡è‹±è¯­æ–‡æ¡£çš„é˜…è¯»éš¾åº¦ï¼š
    æ–‡æ¡£:{text}
    """
    container = st.empty()
    stream_handler = StreamHandler(container)
    prompt =prompt_template.format(text=text)
    llm.predict(prompt,callbacks=[stream_handler])
    container.empty()

    return stream_handler.get_text()

def get_hashValue(text):
    """è®¡ç®—æ–‡æœ¬çš„å“ˆå¸Œå€¼

        Args:
            text(str):æ–‡æœ¬

        Returns:
            hash_value(str):æ–‡æœ¬å“ˆå¸Œå€¼çš„16è¿›å€¼å­—ç¬¦ä¸²
        """
    hash_object = hashlib.sha256(text.encode())
    hash_value = hash_object.hexdigest()
    return hash_value

def ask_doc_questions(text,llm,question):
    """ä½¿ç”¨LLMå›ç­”ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜"""
    prompt_template = """è¯·æ ¹æ®æ–‡æ¡£å›ç­”ä»¥ä¸‹ç›¸å…³é—®é¢˜ï¼š
       æ–‡æ¡£:{text}
       é—®é¢˜:{question}
       """
    container = st.empty()
    stream_handler = StreamHandler(container)
    prompt = prompt_template.format(text=text,question=question)
    llm.predict(prompt, callbacks=[stream_handler])
    container.empty()
    return stream_handler.get_text()



def main():

    '''å°†ä¸€äº›å…³é”®çš„çŠ¶æ€å­˜å‚¨åœ¨session_stateä¸­å¹¶è¿›è¡Œåˆå§‹åŒ–ï¼š
    - explanations:å½“å‰æ–‡æ¡£ç”Ÿè¯çš„é‡Šä¹‰
    - word_freq:å½“å‰æ–‡æ¡£ä¸­æ‰€æœ‰å•è¯çš„é¢‘ç‡
    - current_word_index:å½“å‰å•è¯å¡ç‰‡æ­£åœ¨æ˜¾ç¤ºçš„å•è¯çš„ä¸‹æ ‡
    - unknown_words_list:å½“å‰æ–‡æ¡£ä¸­è¶…å‡ºè®¤çŸ¥èŒƒå›´çš„å•è¯
    - qa_hash:å½“å‰æ–‡æ¡£çš„æ–‡æœ¬å“ˆå¸Œå€¼ï¼Œç”¨äºåœ¨æ–‡æ¡£é—®é¢˜å›ç­”ä¸­åŒºåˆ†æ–°æ—§æ–‡æ¡£
    - df_hash:å½“å‰æ–‡æ¡£çš„æ–‡æœ¬å“ˆå¸Œå€¼ï¼Œç”¨äºåœ¨æ–‡æ¡£éš¾åº¦åˆ†æä¸­åŒºåˆ†ä¸åŒæ–‡æ¡£
    - question:å½“å‰ç”¨æˆ·æå‡ºçš„ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜
    - difficulty:å½“å‰æ–‡æ¡£çš„éš¾åº¦åˆ†æ
    - upload_file:å½“å‰ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£æ–‡ä»¶ã€‘
    - file_uploaded:ç”¨æˆ·æ˜¯å¦ä¸Šä¼ äº†æ–°æ–‡æ¡£
    '''

    if "explanations" not in st.session_state:
        st.session_state.explanations = {}
    if "word_freq" not in st.session_state:
        st.session_state.word_freq = None
    if "current_word_index" not in st.session_state:
        st.session_state.current_word_index = 0
    if "unknown_words_list" not in st.session_state:
        st.session_state.unknown_words_list = []
    if "qa_hash" not in st.session_state:
        st.session_state.qa_hash=None
    if "df_hash" not in st.session_state:
        st.session_state.df_hash=None
    if "question" not in st.session_state:
        st.session_state.question=None
    if "answer" not in st.session_state:
        st.session_state.answer=None
    if "difficulty" not in st.session_state:
        st.session_state.difficulty=None

    if "uploaded_file" not in st.session_state:
        st.session_state.uploaded_file=None

    st.title("ğŸ“š PDFè¯æ±‡å­¦ä¹ åŠ©æ‰‹")
    st.markdown("""
    è¿™æ˜¯ä¸€ä¸ªå¸®åŠ©ä½ æå‡è‹±è¯­è¯æ±‡é‡çš„å·¥å…·:
    1. ä¸Šä¼ è‹±è¯­PDFæ–‡æ¡£
    2. é€‰æ‹©ä½ çš„è¯æ±‡æ°´å¹³
    3. è·å–è¶…å‡ºä½ è¯æ±‡æ°´å¹³çš„å•è¯è§£é‡Šå’Œä¾‹å¥
    """)

    st.divider()

   #ä¸€ä¸ªæ–‡ä»¶ä¸Šä¼ ç»„ä»¶ï¼Œå…è®¸ä¸Šä¼ pdfæˆ–docx
    file = st.file_uploader("ä¸Šä¼ è‹±è¯­PDFæ–‡æ¡£", type=["pdf","docx"],key="file_uploader")


    text=""
    #å¦‚æœæ–‡ä»¶ä¸Šä¼ ç»„ä»¶ä¸­å­˜åœ¨ä¸€ä¸ªæ–‡ä»¶
    if file is not None:
        #æ£€æŸ¥è¿™ä¸ªæ–‡ä»¶æ˜¯å¦æ˜¯æ–°ä¸Šä¼ çš„ï¼Œå¦‚æœæ˜¯åˆ™æ›´æ–°session_stateä¸­çš„çŠ¶æ€ï¼Œåˆ¤å®šç”¨æˆ·ä¸Šä¼ äº†æ–°æ–‡ä»¶
        if file != st.session_state.uploaded_file:
            st.session_state.uploaded_file = file
            st.session_state.file_uploaded = True
        #å¦åˆ™åˆ¤å®šç”¨æˆ·æœªä¸Šä¼ æ–°çš„æ–‡ä»¶
        else:
            st.session_state.file_uploaded = False


    else:
        st.session_state.file_uploaded = False

    # æ ¹æ®ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶çš„æ–‡ä»¶åï¼Œé‡‡ç”¨ç›¸åº”çš„æ–¹æ³•è¯»å–æ–‡ä»¶ï¼Œå¾—åˆ°æ–‡ä»¶å†…å®¹å­—ç¬¦ä¸²text
    if st.session_state.uploaded_file is not None:
        cur_file = st.session_state.uploaded_file
        if cur_file.name.endswith("pdf"):

            with st.spinner('æ­£åœ¨åˆ†ææ–‡æ¡£...'):
                # è¯»å–PDF
                pdf_reader = PdfReader(cur_file)

                for page in pdf_reader.pages:
                    text += page.extract_text()

        elif cur_file.name.endswith("docx"):

            with st.spinner('æ­£åœ¨åˆ†ææ–‡æ¡£...'):
                # è¯»å–PDF
                doc = Document(cur_file)

                for para in doc.paragraphs:
                    text += para.text + "\n"

    #æå–textä¸­çš„æœ‰æ•ˆçš„å•è¯words
    words = extract_words(text)
    st.session_state.word_freq = Counter(words)
    #æ‰¾å‡ºwordsä¸­è¶…å‡ºèŒƒå›´çš„å•è¯
    unknown_words = {word for word in st.session_state.word_freq.keys()
                        if word not in CET6_VOCAB}

    #å½“unknown_wordsä¸ä¸ºç©º
    if unknown_words:
        #ä¸”å½“å‰æ–‡æ¡£æ–‡ä»¶æ˜¯æ–°ä¸Šä¼ çš„
        if st.session_state.file_uploaded:
            with st.spinner('æ­£åœ¨ç”Ÿæˆå•è¯é‡Šä¹‰...'):
                #ä¸ºunknown_wordsä¸­çš„å•è¯ç”Ÿæˆé‡Šä¹‰ï¼Œå­˜å‚¨åœ¨session_stateä¸­
                st.session_state.explanations = get_word_explanations(unknown_words, llm)
                st.session_state.unknown_words_list = sorted(unknown_words)


        #å¹¶åˆ—æ”¾ç½®ä¸‰ä¸ªå¯¼èˆªæŒ‰é’®ç”¨äºéå†unknown_words
        col1, col2, col3 = st.columns(3)
        #é€šè¿‡ä¿®æ”¹çŠ¶æ€ä¸­çš„current_word_indexï¼Œå®ç°ç‚¹å‡»æŒ‰é’®æ˜¾ç¤ºä¸Šä¸€ä¸ªæˆ–ä¸‹ä¸€ä¸ªå•è¯
        with col1:
            if st.button("â¬…ï¸ ä¸Šä¸€ä¸ª"):
                st.session_state.current_word_index = (st.session_state.current_word_index - 1) % len(
                    st.session_state.unknown_words_list)
        with col2:
            if st.button("â¡ï¸ ä¸‹ä¸€ä¸ª"):
                st.session_state.current_word_index = (st.session_state.current_word_index + 1) % len(
                    st.session_state.unknown_words_list)
        with col3:
            if st.button("ğŸ”„ é‡ç½®"):
                st.session_state.current_word_index = 0

        # ä»ç¼“å­˜ä¸­è·å–å½“å‰å•è¯ä¸‹æ ‡å¹¶æ˜¾ç¤ºå½“å‰å•è¯å¡ç‰‡
        if st.session_state.current_word_index>=len(st.session_state.unknown_words_list):
            st.session_state.current_word_index=0
        current_word = st.session_state.unknown_words_list[st.session_state.current_word_index]
        display_word_card(
            current_word,
            st.session_state.explanations[current_word],
            st.session_state.word_freq[current_word],
            st.session_state.current_word_index,
            len(st.session_state.unknown_words_list),
            True
        )

    st.divider()
    difficulty_btn = st.button("åˆ†ææ–‡æ¡£é˜…è¯»éš¾åº¦")
    #å¦‚æœå½“å‰æ–‡æ¡£æ–‡æœ¬ä¸ä¸ºç©º
    if len(text)>0:
        if difficulty_btn:
            #ç‚¹å‡»æŒ‰é’®åï¼Œæ¯”è¾ƒsession_stateä¸­çš„æ–‡æœ¬å“ˆå¸Œå€¼ä¸å½“å‰æ–‡æœ¬çš„å“ˆå¸Œå€¼ï¼Œåˆ¤æ–­ä¸¤ä¸ªæ–‡æœ¬æ˜¯å¦ä¸€è‡´
            if not (get_hashValue(text) == st.session_state.df_hash and st.session_state.difficulty):
                #å½“ä¸¤ä¸ªæ–‡æœ¬ä¸ä¸€è‡´æˆ–session_stateä¸­çš„éš¾åº¦åˆ†ææœªç”Ÿæˆæ—¶ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆéš¾åº¦åˆ†æ
                with st.spinner('æ­£åœ¨åˆ†ææ–‡æ¡£éš¾åº¦...'):
                    difficulty =get_doc_difficulty(text,llm)
                    st.session_state.difficulty=difficulty
                    st.session_state.df_hash = get_hashValue(text)
                #å¦åˆ™ï¼Œç›´æ¥æ˜¾ç¤ºå·²æœ‰çš„éš¾åº¦åˆ†æ
        if get_hashValue(text) == st.session_state.df_hash and st.session_state.difficulty:
            st.markdown(st.session_state.difficulty)



    st.divider()

    input_container = st.empty()
    #ä¸€ä¸ªæ–‡æœ¬è¾“å…¥æ¡†ç”¨äºè¾“å…¥é—®é¢˜
    user_input = input_container.text_input("æé—®ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜",key="text_input",)
    if st.session_state.file_uploaded:
        #å½“æ–°ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶æ—¶ï¼Œå°†è¾“å…¥æ¡†æ¸…ç©º
        user_input = input_container.text_input("æé—®ä¸æ–‡æ¡£ç›¸å…³çš„é—®é¢˜",key="text_input_empty",value='')
    if len(text)>0:
        #å¦‚æœå½“å‰æ–‡æœ¬ä¸ä¸ºç©º
        if len(user_input)>0:
            #å¦‚æœå½“å‰è¾“å…¥çš„é—®é¢˜å’Œsession_stateä¸­ä¿å­˜é—®é¢˜ä¸€è‡´ï¼Œå½“å‰æ–‡æœ¬çš„å“ˆå¸Œå€¼ä¸session_stateä¸­ä¿å­˜çš„æ–‡æœ¬å“ˆå¸Œå€¼ä¸€è‡´ä¸”session_stateä¸­å·²æœ‰ç­”æ¡ˆ
            if user_input == st.session_state.question and st.session_state.answer and get_hashValue(text)==st.session_state.qa_hash:
                #ç›´æ¥æ˜¾ç¤ºsession_stateç¼“å­˜çš„ç­”æ¡ˆ
                st.write(st.session_state.question)
                st.write(st.session_state.answer)
            else:
                #å¦åˆ™é‡æ–°ç”Ÿæˆç­”æ¡ˆï¼Œå¹¶æ›´æ–°session_state
                with st.spinner('æ­£åœ¨æ€è€ƒæ–‡æ¡£é—®é¢˜...'):
                    st.session_state.question = user_input
                    response = ask_doc_questions(text,question=user_input,llm=llm)
                    st.session_state.answer=response
                    st.session_state.qa_hash=get_hashValue(text)
                    #st.write(st.session_state.question)
                    st.write(st.session_state.answer)



def recall_new_words():
    if "current_recall_word_index" not in st.session_state:
        st.session_state.current_recall_word_index = 0

    num_words = st.session_state.num_new_words
    st.header("éœ€è¦å¤ä¹ çš„ç”Ÿè¯:{}".format(num_words))

    query_sql = "SELECT word , explanations FROM new_words ORDER BY insert_time LIMIT ?"

    res = exec_query(query_sql, (num_words,))
    new_words = []
    explanations = {}

    for row in res:
        new_words.append(row[0])
        explanations[row[0]] = row[1]


    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("â¬…ï¸ ä¸Šä¸€ä¸ª"):
            st.session_state.current_recall_word_index = (st.session_state.current_recall_word_index - 1) % len(
                new_words)
    with col2:
        if st.button("â¡ï¸ ä¸‹ä¸€ä¸ª"):
            print("ä¸‹ä¸€ä¸ª:{}".format(st.session_state.current_recall_word_index))
            st.session_state.current_recall_word_index = (st.session_state.current_recall_word_index + 1) % len(
                new_words)
    with col3:
        if st.button("ğŸ”„ é‡ç½®"):
            st.session_state.current_recall_word_index = 0

    # ä»ç¼“å­˜ä¸­è·å–å¹¶æ˜¾ç¤ºå½“å‰å•è¯å¡ç‰‡
    if st.session_state.current_word_index >= len(new_words):
        st.session_state.current_word_index = 0
    current_word = new_words[st.session_state.current_recall_word_index]
    display_word_card(
        current_word,
        explanations[current_word],
        1,
        st.session_state.current_recall_word_index,
        len(new_words),
        False
    )




if __name__ == "__main__":

    if "page" not in st.session_state:
        st.session_state.page = "PDF"

    if "num_new_words" not in st.session_state:
        st.session_state.num_new_words = 0
    #åˆå§‹åŒ–session_stateä¸­å…³äºç”¨æˆ·uidå’Œç”¨æˆ·å•è¯è¡¨çš„çŠ¶æ€
    if "word_lists" not in st.session_state:
        st.session_state.word_lists={}
    #ä»urlä¸­è¯»å–ä¼ å…¥çš„ç”¨æˆ·uid
    params = st.query_params
    if "uid" in params:
        st.session_state.uid = params['uid'][0]
        #ä»æ•°æ®åº“ä¸­æŸ¥è¯¢ç”¨æˆ·åˆ›å»ºçš„å•è¯è¡¨
        word_lists=db.get_user_wordlists(params['uid'][0])
        #å°†å•è¯è¡¨å­˜å‚¨åœ¨session_stateçš„å­—å…¸ä¸­ï¼Œä»¥å•è¯è¡¨çš„nameä¸ºé”®ï¼Œä»¥å•è¯è¡¨çš„idä¸ºå€¼
        for word_list in word_lists:
            st.session_state.word_lists[word_list["name"]] = word_list["wordlist_id"]



    interval = 1
    main_container = st.empty()
    modal = Modal(key="modal_Key", title="è­¦å‘Š")
    page = st.selectbox("é€‰æ‹©åŠŸèƒ½", ["PDF", "å¤ä¹ å•è¯"])

    if page == "PDF":
        st.session_state.page = "PDF"
    elif page == "å¤ä¹ å•è¯":
        if st.session_state.num_new_words > 0:
            st.session_state.page = "recall_words"
        else:
            with modal.container():
                st.write("è¯·è¾“å…¥æ­£ç¡®çš„å•è¯æ•°é‡ï¼")



    #æ˜¾ç¤ºä¾§è¾¹æ 
    with st.sidebar:
        st.header("è®¾ç½®")
        level = st.selectbox(
            "é€‰æ‹©ä½ çš„è¯æ±‡æ°´å¹³",
            ["CET6"],
            help="ç›®å‰ä»…æ”¯æŒCET6æ°´å¹³æ£€æµ‹"
        )

        num_words = int(st.number_input("è¾“å…¥è¦å¤ä¹ çš„ç”Ÿè¯æ•°é‡"))
        st.session_state.num_new_words = num_words


    if st.session_state.page == "PDF":
        #è°ƒç”¨mainæ–¹æ³•æ˜¾ç¤ºä¸»é¡µé¢
        main()
    elif st.session_state.page == "recall_words":
        recall_new_words()
